# -*- coding: utf-8 -*-
"""mzy2001_kaggle.ipynb

Automatically generated by Colaboratory.
"""

import numpy as np
import pandas as pd 
from os import listdir
from numpy import zeros
from numpy import asarray
from numpy import savez_compressed
from pandas import read_csv
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from numpy import mean
from numpy import std
from numpy import load
from matplotlib import pyplot
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from keras.preprocessing.image import ImageDataGenerator
import cv2
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Input, MaxPool2D, Conv2D, Flatten, LeakyReLU, BatchNormalization,Activation, concatenate, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.initializers import Constant
from tensorflow.keras.applications.resnet_v2 import ResNet50V2
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from sklearn.metrics import confusion_matrix
import itertools
from matplotlib import pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score
from keras.models import load_model



def load_dataset_test(path, maps):
    photos = list()
    name = []
    # enumerate files in the directory
    for filename in listdir(path):
        # load image
        photo = load_img(path + filename, target_size=(128,128))
        # convert to numpy array
        photo = img_to_array(photo, dtype='uint8')
        name.append(maps["test/"+filename])
        # store
        photos.append(photo)
    X = np.array(photos)
    name = np.array(name)
    return X, name

def load_train_dataset(path, mapping):
    photos, targets = list(), list()
    # enumerate files in the directory
    for filename in listdir(path):
        # load image
        photo = load_img(path + filename, target_size=(128, 128))
        # convert to numpy array
        photo = img_to_array(photo, dtype='uint8')
        #photo = remove_text(photo)
        # get tags
        label = mapping['train/'+filename]
        # one hot encode tags
        target = [0]*4
        target[label] = 1
        # store
        photos.append(photo)
        targets.append(target)
    X = asarray(photos, dtype='uint8')
    y = asarray(targets, dtype='uint8')
    return X, y


# load train and test dataset
def load_dataset():
	# load dataset
  data = load('covid_data_128_color.npz')
  X, y = data['arr_0'], data['arr_1']
	# separate into train and test datasets
  trainX, testX, trainY, testY = train_test_split(X, y, 
                                                test_size=0.1, random_state=seed, 
                                                stratify = y)  
  print(trainX.shape, trainY.shape, testX.shape, testY.shape)
  return trainX, trainY, testX, testY


# scale pixels
def prep_pixels(train, test):
	# convert from integers to floats
	train_norm = train.astype('float32')
	test_norm = test.astype('float32')
	# normalize to range 0-1
	train_norm = train_norm / 255.0
	test_norm = test_norm / 255.0
	# return normalized images
	return train_norm, test_norm

def remove_text(img):
    mask = cv2.threshold(img, 230, 255, cv2.THRESH_BINARY)[1].astype(np.uint8)
    img = img.astype(np.uint8)
    result = cv2.inpaint(img, mask, 10, cv2.INPAINT_NS).astype(np.float32)
    return result


def get_class_weights(histogram):
  weights = [None] * len(histogram)
  for i in range(len(histogram)):
      weights[i] = (1.0 / len(histogram)) * sum(histogram) / histogram[i]
  class_weight = {i: weights[i] for i in range(len(histogram))}
  print("Class weights: ", class_weight)
  return class_weight


def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    
    fig = plt.figure(figsize=(10,10))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

def getPreds(model):
  data = load('covid_data_test_128.npz')
  XTEST = data['arr_0']
  nums = data['arr_1']
  print(nums)
  XTEST = XTEST.astype('float32')
  XTEST = XTEST / 255.0
  preds = model.predict(XTEST)
  preds = [np.argmax(a) for a in preds]
  sorted_preds = [None] * len(preds)
  for i in range(len(preds)):
    sorted_preds[nums[i]] = preds[i] 
  print(sorted_preds)
  return sorted_preds

def toSubmit(preds, classes):
    preds = [classes[x] for x in preds]
    df = pd.DataFrame({'Id': range(len(preds)), 'label' : preds})
    df.to_csv("submission.csv", index=False)


def CNN_model():
  model = Sequential(input_shape = (128,128,1), num_classes=4)

  model.add(Conv2D(filters=64, kernel_size=(5, 5), input_shape=input_shape, activation='relu'))
  model.add(BatchNormalization(axis=3))
  model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(BatchNormalization(axis=3))
  model.add(Dropout(0.1))

  model.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu'))
  model.add(BatchNormalization(axis=3))
  model.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(BatchNormalization(axis=3))
  model.add(Dropout(0.1))

  model.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu'))
  model.add(BatchNormalization(axis=3))
  model.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(BatchNormalization(axis=3))
  model.add(Dropout(0.1))

  model.add(Flatten())

  model.add(Dense(256, activation='relu'))
  model.add(BatchNormalization())
  model.add(Dropout(0.5))

  model.add(Dense(256, activation='relu'))
  model.add(BatchNormalization())
  model.add(Dropout(0.5))

  model.add(Dense(num_classes, activation='softmax'))

  model.summary()
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  return model


def ResNet50_model(input_shape=(128,128,3)):
  optimizer = SGD(lr=0.01)

  X_input = Input(input_shape, name='input_img')
  base_model = ResNet50V2(include_top=False, weights='imagenet', input_shape=input_shape, input_tensor=X_input)
  X = base_model.output

  X = GlobalAveragePooling2D()(X)
  X = BatchNormalization()(X)
  X = Dropout(0.2)(X)
  X = Dense(512, kernel_initializer='he_uniform')(X)
  X = LeakyReLU()(X)
  X = Dense(4)(X)
  Y = Activation('softmax', dtype='float32', name='output')(X)

  model = Model(inputs=X_input, outputs=Y)
  model.summary()
  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
  return model










# read csv and get mappings
classes = ['normal', 'bacterial', 'viral', 'covid']
int_mapping = dict(zip(classes, [0,1,2,3]))

df = pd.read_csv('/kaggle/input/4771-sp20-covid/train.csv')
labels = np.array(df.label)
mapping = dict(zip(df.filename,[int_mapping[x] for x in df.label]))

df = pd.read_csv("../input/4771-sp20-covid/test.csv")
maps = dict(zip(df.filename, df.id))



# generate train file
path = "../input/4771-sp20-covid/train/train/"
X, y = load_dataset(path, mapping)
print(X.shape, y.shape)
savez_compressed('covid_data_128_color.npz', X, y)



# generate test file
path = "../input/4771-sp20-covid/test/test/"
X,name = load_dataset_test(path, maps)
print(X.shape, name.shape)
savez_compressed('covid_data_test_128_color.npz', X, name)



seed = 7 
np.random.seed(seed)

# get train test split
trainX, trainY, testX, testY = load_dataset()
trainX, testX = prep_pixels(trainX, testX)

# data augmentation
datagen = ImageDataGenerator(rotation_range=30, width_shift_range=0.2, horizontal_flip=True, zoom_range=0.2, shear_range=0.2)
datagen.fit(trainX)

# class weights
histogram = np.bincount(np.array([np.argmax(x) for x in trainY]).astype(int))  # Get class distribution
class_weight = get_class_weights(histogram)

# learning rate reduction
learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', 
                                          patience=3, 
                                          verbose=1, 
                                          factor=0.4, 
                                          min_lr=0.00001)

# checkpoints
filepath="weights.best_{epoch:02d}-{val_accuracy:.2f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', 
                            verbose=1, save_best_only=True, mode='max')
filepath="weights.last_auto4.hdf5"
checkpoint_all = ModelCheckpoint(filepath, monitor='val_accuracy', 
                                verbose=1, save_best_only=False, mode='max')

# all callbacks
callbacks_list = [checkpoint, learning_rate_reduction, checkpoint_all]

# get model
model = ResNet50_model()
#model = CNN_model()

# fit model
hist = model.fit_generator(datagen.flow(trainX, trainY, batch_size=32), 
                           epochs=20, validation_data=(testX, testY), 
                           steps_per_epoch=trainX.shape[0], callbacks=callbacks_list, class_weight=class_weight)




model.load_weights("weights.best_11-0.87.hdf5")

print(model.evaluate(trainX, trainY))  # Evaluate on train set
print(model.evaluate(testX, testY))  # Evaluate on test set

# Predict the values from the validation dataset
predY = model.predict(testX)
predYClasses = np.argmax(predY, axis = 1) 
trueY = np.argmax(testY, axis = 1) 

# confusion matrix
confusionMTX = confusion_matrix(trueY, predYClasses) 
# plot the confusion matrix
plot_confusion_matrix(confusionMTX, classes = classes) 

precision = precision_score(trueY, predYClasses, average='weighted')
recall = recall_score(trueY, predYClasses, average='weighted')
f1 = f1_score(trueY, predYClasses, average="weighted")
print("Precision: ", precision)
print("Recall: ", recall)
print("F1: ", f1)


# get predictions and submission file
preds = getPreds(model)
toSubmit(preds, classes)